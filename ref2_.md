# 데이터 정규화 & 표준화
데이터의 어떤 `경향`을 제거하여 동등한 환경의 데이터를 만드는 작업 

## 1. 정규화
![](/assets/nor.png)
* 전체 구간을 0~100으로 설정하여 데이터를 관찰 
* 데이터 군 내에서 특정 데이터가 가지는 위치 관찰

## 2. 표준화
![](/assets/stamd.png)
* 평균을 기준으로 얼마나 떨어져 있는지 관찰
* 2개 이상의 데이터 단위가 다를때 대상 데이터를 같은 기준으로 볼수 있게 함(나이 vs 키)

# 데이터의 확장 (Data Augmentation)


# 여러 신경망의 평균 (=Model Averaging)
* 여러개의 서로 다른 신경망을 조합하면 일반적으로 추정의 정확도를 향상시킬수 있다. 
* 같은 입력을 여러개의 신경망에 입력하여 얻어진 출력의 평균을 응답으로 삼는다. 
* 신경망은 `서로다른 구조` or `같은 구조, 다른 초기값` 등 

* 단점, 복수의 신경망을 학습 시켜야 함으로 시간 및 연산량이 증가

> __드롭아웃__은 신경망 하나를 가용해서 실질적으로 여러개의 신경망에 모델 평균을 적용한것과 같은 효과 있다로 여겨지고 있음

# 학습률 결정(Learning Rate)
## 1. 인의적 결정

### 1.1 
학습 초기에 값을 크게 설정 했다가 학습의 진행과 함께 낮추어 감

### 1.2 
각 층마다 서로 다른 학습률을 적용 
단, 램프함수에는 적합하지 않음 

## 2. 자동 결정

## 2.1 AdaGrad 
* 자주 나타나는 기울기의 성분보다 드물게 나타나는 기울기 성분을 더 둥시해서 파라미터를 업데이트 
* 현재 거의 일반적으로 사용됨 
 

# 미니배치(Minibatch)
* 샘플한개 단위가 아니라 몇 개의 샘플을 하나의 작은 집합으로 묶은 집합 단위로 가중치를 업데이트 한다. 
* 복수의 샘플을 묶은 작은 집합을 `미니배치`라고 부른다. 

> 참고 : 딥러닝 제대로 시작하기 35P


#Ensemble 
* 여러 학습 모델을 생성하고 마지막에 합쳐서 결과를 산출
* 2~4,5%까지 성능 향상 가능
* 충분한 컴퓨팅 파워 필요 





