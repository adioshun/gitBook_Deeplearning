
# 데이터의 백색화(Whitening)
* 정규화 보다 수준 높은 데이터의 `경향`을 제거 하는 작업
* 차이 : 정규화는 샘플의 성분 단위 처리, 백색화는 성분 간의 관계를 수정 하는 처리
* 백색화는 자기부호화기가 좋은 자질을 학습할 수 있을지를 크게 좌우 할수 있다.
* 목적 : 훈련샘플에서 성분간의 __상관성__을 제거


# 데이터의 확장 (Data Augmentation)
...

# 여러 신경망의 평균 (=Model Averaging)
* 여러개의 서로 다른 신경망을 조합하면 일반적으로 추정의 정확도를 향상시킬수 있다.
* 같은 입력을 여러개의 신경망에 입력하여 얻어진 출력의 평균을 응답으로 삼는다.
* 신경망은 `서로다른 구조` or `같은 구조, 다른 초기값` 등

* 단점, 복수의 신경망을 학습 시켜야 함으로 시간 및 연산량이 증가

> __드롭아웃__은 신경망 하나를 가용해서 실질적으로 여러개의 신경망에 모델 평균을 적용한것과 같은 효과 있다로 여겨지고 있음


# 학습률 결정(Learning Rate)
경사 하강법에서의 파라미터의 업데이트 정도 정의 방법
## 1. 인의적 결정
### 1.1
학습 초기에 값을 크게 설정 했다가 학습의 진행과 함께 낮추어 감

### 1.2
각 층마다 서로 다른 학습률을 적용
* 출력 방향에 가까운 얕은 층에서는 학습률을 작게 잡고, 입력에 가까운 깊은 층에는 크게 잡기
단, 램프함수에는 적합하지 않음

## 2. 자동 결정
### 2.1 AdaGrad
* 자주 나타나는 기울기의 성분보다 드물게 나타나는 기울기 성분을 더 둥시해서 파라미터를 업데이트
* 현재 거의 일반적으로 사용됨
### 2.2 모멘텀(Momentum)
* 경사 하강법의 수렴 성능을 향상시키기 위한 방법
* 오차 함수의 골짜기 바닥이 평평할 경우에는 성능이 떨어진다.
* 골짜기 바닥을 조금만 빠져 나오면 골짜기와 직교 하는 방향으로 큰 기울기가 생긴다.
* 그 결과 가중치는 매번 골짜기가 직교 하는 방향으로 업데이트 되어서 경로가 지그재그 모양이 되고 정상적으로 최저점을 찾지 못한다.
### 2.3 Adam = AdaGrad + Momentum

> 참고 : 딥러닝 제대로 시작하기 49page

# 가중치의 초기화
## 1. 인의적 결정
* 가우스 분포로부터 랜덤값을 생성하여 초기값으로 설정
* 바이어스 초기값은 통상 `0`으로 설정

## 2. 사전 훈련을 통해서 결정
* 딥뉴럴넷에서는 이 방법이 더 일반적이다.

## 3. 최근 추세
- 활성함수로 Sigmod / Tanh 사용시 `Xavier` 초기방식 사용
- 활성함수로 Relu 사용시 `He` 초기방식 사용

# 샘플의 순서
* 미니배치를 쓸때 어떤 샘플을 조합하여 구성할지, 어떤 순서로 제시 할지 자율
* 일반적으로 신경망에 익숙하지 않는 샘플을 먼저 제시 (=아직 잘 학습되지 않은 샘플 먼저)

* `딥뉴럴넷`에서는 클래스간에 샘플수에 편차가 없도록 셔플링한 샘플을 기계적으로 조합하여 미니패치를 구성하고, 구성한 순서대로 반복하여 신경망에 입력 하는 경우가 많다

# 미니배치(Minibatch)
* 샘플한개 단위가 아니라 몇 개의 샘플을 하나의 작은 집합으로 묶은 집합 단위로 가중치를 업데이트 한다.
* 복수의 샘플을 묶은 작은 집합을 `미니배치`라고 부른다.

배치사이즈는 몇 문항을 풀고 해답을 맞추는 지를 의미합니다.
- 100문항일 때, 배치사이즈가 100이면 전체를 다 풀고 난 뒤에 해답을 맞춰보는 것입니다
- 100문항 다 풀고 해답과 맞추어보려면 문제가 무엇이었는지 다 기억을 해야 맞춰보면서 학습이 되겠죠? 기억력(용량)이 커야합니다
- 100문항일 때, 배치사이즈가 10이면 열 문제씩 풀어보고 해답 맞춰보는 것입니다. 100문항을 10문제씩 나누어서 10번 해답을 맞추므로 가중치 갱신은 10번 일어납니다.
- 1문항씩 풀고 해답 맞추면 학습은 꼼꼼히 잘 되겠지만 시간이 너무 걸리겠죠

목적 : 문제를 푼 뒤 해답과 맞춰봐야 학습이 일어납니다. > 모델의 결과값과 주어진 라벨 값과의 오차를 줄이기 위해, `역전파(Backpropagation)` 알고리즘으로 가중치가 갱신됩니다.
0


> 참고 : 딥러닝 제대로 시작하기 35P

> [텐서플로우 배치 처리](http://bcho.tistory.com/1170)

---
* [Improving the way neural networks learn](http://neuralnetworksanddeeplearning.com/chap3.html)




